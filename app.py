import streamlit as st
import torch
import json
from src.preprocess import tokenize, build_vocab, encode, decode, prepare_input, filter_prompts
from src.model import Seq2SeqModel
from src.infer import generate_response

# Chargement des donn√©es pour suggestions dynamiques
with open("data/prompts_completions.jsonl", "r", encoding="utf-8") as f:
    prompts_data = [json.loads(line)["prompt"] for line in f]

# Chemins vers le vocabulaire et le mod√®le sauvegard√©
vocab_path = "models/vocab.json"
model_path = "models/seq2seq_model.pth"

@st.cache_data
def load_resources():
    """
    Charge le vocabulaire, initialise le mod√®le Seq2Seq et charge ses poids.
    V√©rifie la pr√©sence des tokens sp√©ciaux n√©cessaires.
    """
    with open(vocab_path, "r", encoding="utf-8") as f:
        vocab = json.load(f)

    vocab = {k: int(v) for k, v in vocab.items()}
    idx2word = {v: k for k, v in vocab.items()}

    required_tokens = ["<pad>", "<sos>", "<eos>", "<unk>"]
    for token in required_tokens:
        if token not in vocab:
            st.error(f"Le token sp√©cial '{token}' est manquant dans le vocabulaire.")
            st.stop()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = Seq2SeqModel(vocab_size=len(vocab), embed_dim=128, hidden_dim=256, pad_idx=vocab["<pad>"])
    model = model.to(device)

    try:
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
    except RuntimeError as e:
        st.error(f"Erreur de chargement du mod√®le : {e}")
        st.stop()

    return vocab, idx2word, model, device

# Chargement initial des ressources
vocab, idx2word, model, device = load_resources()

# Titre principal
st.title("üîÆ Compl√©tion intelligente de phrases administratives")

with st.expander("‚ÑπÔ∏è √Ä propos de l'application"):
    st.markdown("""
    Cette application utilise un mod√®le de type **Seq2Seq** entra√Æn√© sur un jeu de paires `prompt ‚Üí compl√©tion`
    pour g√©n√©rer automatiquement des phrases administratives compl√®tes √† partir de d√©buts de phrase.
    
                
    üí° Commencez √† taper votre demande, cliquez sur une suggestion ou lancez la compl√©tion automatique.
    """)

if "user_input" not in st.session_state:
    st.session_state["user_input"] = ""

if "auto_complete" not in st.session_state:
    st.session_state["auto_complete"] = False

# Champ texte pour la saisie
user_input = st.text_input("Saisissez une demande incompl√®te...", st.session_state["user_input"])

# Suggestions dynamiques
suggestions = filter_prompts(user_input, prompts_data)[:5]
if suggestions:
    st.markdown("**üí° Suggestions tir√©es du jeu d'entra√Ænement :**")
    for i, suggestion in enumerate(suggestions):
        if st.button(suggestion, key=f"suggestion_btn_{i}"):
            st.session_state["user_input"] = suggestion
            st.session_state["auto_complete"] = True
            st.rerun()

# Compl√©tion manuelle
if st.button("‚ú® Compl√©ter la demande") and user_input:
    st.session_state["user_input"] = user_input
    st.session_state["auto_complete"] = True
    st.rerun()

# G√©n√©ration automatique apr√®s clic sur suggestion ou bouton
if st.session_state.get("auto_complete") and st.session_state["user_input"]:
    try:
        result = generate_response(
            st.session_state["user_input"], model, vocab, idx2word, device=device
        )
        st.markdown("### ‚úçÔ∏è Compl√©tion propos√©e :")
        st.success(result)

        with st.expander("üîç D√©tails internes (tokens encod√©s)"):
            tokens = tokenize(st.session_state["user_input"])
            indices = [vocab.get(tok, vocab["<unk>"]) for tok in tokens]
            st.write("Tokens :", tokens)
            st.write("Indices :", indices)

    except Exception as e:
        st.error(f"Une erreur est survenue pendant la g√©n√©ration : {e}")
    finally:
        st.session_state["auto_complete"] = False  # R√©initialiser

